{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06319223-e3e9-4fb1-8ff6-2dfb5f21f64c",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Blue;\">Thyroid Prediction Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd950cf-6c21-424f-aeea-c9c528e34c2c",
   "metadata": {},
   "source": [
    "<img src=\"images/Thyroid.png\" alt=\"Image About Thyroid\" width=\"200\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d597977-6f42-4ed9-bce5-7d7fbf6e3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Predictive analysis template for an attached CSV (thyroid.csv)\n",
    "\n",
    "###  - Loads the file\n",
    "###  - Auto-detects a plausible target column\n",
    "###  - Handles preprocessing for numeric & categorical features\n",
    "###  - Trains baseline models (classification or regression depending on target type)\n",
    "###  - Evaluates with clear metrics\n",
    "###  - Produces multiple Matplotlib charts (no seaborn, one plot per figure, no explicit colors)\n",
    "###  - Saves trained model and metrics to /mnt/data for download\n",
    "### \n",
    "###  This script is defensive: it tries to \"do the right thing\" regardless of schema.\n",
    "###  You can safely re-run it after tweaking TARGET_COLUMN below if you prefer a different target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d510f-2c74-48de-b859-8f37dbd09283",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Install Packages / Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0123a87-0b65-46d1-923a-a968127b53db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing scikit-learn ... ‚úÖ\n",
      "üì¶ Installing matplotlib ... ‚úÖ\n",
      "üì¶ Installing numpy ... ‚úÖ\n",
      "üì¶ Installing pandas ... ‚úÖ\n",
      "üì¶ Installing seaborn ... ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries:\n",
    "import subprocess, sys\n",
    "\n",
    "packages = [\"scikit-learn\", \"matplotlib\", \"numpy\", \"pandas\", \"seaborn\"]\n",
    "\n",
    "for pkg in packages:\n",
    "    print(f\"üì¶ Installing {pkg} ...\", end=\"\", flush=True)\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"],\n",
    "        stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n",
    "    )\n",
    "    print(\" ‚úÖ\")\n",
    "\n",
    "# !pip install scikit-learn --force-reinstall -q > /dev/null 2>&1\n",
    "# !pip install matplotlib --force-reinstall -q > /dev/null 2>&1\n",
    "# !pip install pandas --force-reinstall -q > /dev/null 2>&1\n",
    "# !pip install numpy --force-reinstall -q > /dev/null 2>&1\n",
    "# !pip install seaborn --force-reinstall -q > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc44b51-2750-4929-9edd-45eba8125c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn Version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "##  Import the required Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# -------------          Statistics Libraries          -------------     \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, \\\n",
    "                            roc_curve, confusion_matrix, ConfusionMatrixDisplay, classification_report, \\\n",
    "                            r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "# Checking sklearn version\n",
    "import sklearn\n",
    "print(f\"sklearn Version: {sklearn.__version__}\")\n",
    "#\n",
    "import seaborn as sns\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "#-------------          Other Libraries               -------------  \n",
    "#------------------------------------------------------------------\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb503a87-8ee3-4346-b778-b06ce24cfab5",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red;\">Configuration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d30b5fc-5e37-410a-828b-7c35cca483a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Path Location\n",
    "DATA_PATH = './datasets/thyroid.csv'\n",
    "\n",
    "# Output Data Analysis\n",
    "OUTPUT_DIR = Path(\"./analysis/thyroid_analysis_outputs\")\n",
    "# Conf Directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "#\n",
    "# Optionally hard-set the target (uncomment & set if you already know it)\n",
    "TARGET_COLUMN = None  # e.g., \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "034f9224-a3dc-4277-8c75-381a49eae803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Thyroid CSV File\n",
    "tdata = pd.read_csv('./datasets/thyroid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f4bbb-8b09-40ad-bf57-831799e8bcb3",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red;\">Functions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1640915b-34b6-412a-af01-20088dce5455",
   "metadata": {},
   "source": [
    "<h3 style=\"color:Blue;\">choose_target_column</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e86954-19a4-458e-b113-4a8b0aafb24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_target_column(df: pd.DataFrame) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "        Choose a plausible target column from the dataframe.\n",
    "        Returns: (target_col, problem_type) where problem_type ‚àà {\"classification\",\"regression\"}\n",
    "        Heuristics:\n",
    "              1) Prefer common names: ['target','class','label','diagnosis','outcome','y']\n",
    "              2) Otherwise, pick a column with small cardinality (2..10 unique) => classification\n",
    "              3) Otherwise, pick the last column; determine type by numeric vs non-numeric\n",
    "    \"\"\"\n",
    "    candidates = [c for c in df.columns if c.lower() in\n",
    "              [\"target\", \"class\", \"label\", \"diagnosis\", \"outcome\", \"y\"]]\n",
    "\n",
    "    for c in candidates:\n",
    "        nuniq = df[c].nunique(dropna=True)\n",
    "        if 2 <= nuniq <= 10:\n",
    "            return c, \"classification\"\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            return c, \"regression\"\n",
    "        return c, \"classification\"\n",
    "\n",
    "    small_card_cols = []\n",
    "    for c in candidates:\n",
    "        if c == df.columns[0]:\n",
    "            continue\n",
    "\n",
    "        nuniq = df[c].nunique(dropna=True)\n",
    "\n",
    "        if 2 <= nuniq <= 10:\n",
    "            small_card_cols.append((c, nuniq))\n",
    "    if small_card_cols:\n",
    "        small_card_cols.sort(key=lambda x: x[1])\n",
    "        return small_card_cols[0][0], \"classification\"\n",
    "\n",
    "    c = df.columns[-1]\n",
    "    if pd.api.types.is_numeric_dtype(df[c]):\n",
    "        if df[c].nunique(dropna=True) > 15:\n",
    "            return c, \"regression\"\n",
    "        return c, \"classification\"\n",
    "    else:\n",
    "        return c, \"classification\"\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203756b5-7845-4cc0-aebb-451819555bc0",
   "metadata": {},
   "source": [
    "<h3 style=\"color:Blue;\">split_features_target</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd11a059-7b68-4d29-8267-aabb20f32f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_target(df: pd.DataFrame, target_col: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    y = df[target_col]\n",
    "    X = df.drop(columns=[target_col])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c2b7e-bb9e-4844-b15a-eb9892b11732",
   "metadata": {},
   "source": [
    "<h3 style=\"color:Blue;\">_make_ohe</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e3d8e26-dfca-45bf-b14a-96925a94b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def _make_ohe():\n",
    "    try:\n",
    "        # scikit-learn >= 1.2 (usa sparse_output)\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    except TypeError:\n",
    "        # scikit-learn <= 1.1 (usa sparse)\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3b1d1-2403-49f8-b357-1c5fb0c26fa3",
   "metadata": {},
   "source": [
    "<h3 style=\"color:Blue;\">build_preprocessor</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e947d13c-6b5a-4791-8ea1-256ed89724ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def build_preprocessor(X: pd.DataFrame) -> Tuple[ColumnTransformer, List[str], List[str]]:\n",
    "    numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "    categorical_features = [c for c in X.columns if c not in numeric_features]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    return preprocessor, numeric_features, categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f65ebb-478f-432a-afcc-01b306772c64",
   "metadata": {},
   "source": [
    "<h3 style=\"color:Blue;\">plot_roc_curve_binary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1778a842-923d-4868-ad28-6dd9e823946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve_binary(y_true, y_proba, title: str, out_path: Path):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=\"ROC\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36124948-c53e-465f-b0c3-b32e11bce925",
   "metadata": {},
   "source": [
    "<h3 style=\"color:Blue;\">plot_confusion</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1c01682-d935-4384-9978-6eae208bd01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(y_true, y_pred, title: str, out_path: Path, labels=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    plt.figure()\n",
    "    disp.plot(values_format=\"d\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ed456-5805-490f-a0a2-4c361e4e3a10",
   "metadata": {},
   "source": [
    "<h3 style=\"color:Blue;\">plot_feature_importance</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fff7573-f85d-4405-8a55-96b08dc2e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names: List[str], top_k: int, title: str, out_path: Path):\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "        idx = np.argsort(importances)[::-1][:top_k]\n",
    "        plt.figure()\n",
    "        plt.bar(range(len(idx)), importances[idx])\n",
    "        plt.xticks(range(len(idx)), [feature_names[i] for i in idx], rotation=90)\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_path)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1632e-6580-4a6a-9447-d37969ba0293",
   "metadata": {},
   "source": [
    "<h3 style=\"color:Blue;\">plot_scatter_true_pred</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "346947fd-0848-4766-aa07-497f77feb3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_true_pred(y_true, y_pred, title: str, out_path: Path):\n",
    "    plt.figure()\n",
    "    plt.scatter(y_true, y_pred, alpha=0.7)\n",
    "    min_v = min(np.min(y_true), np.min(y_pred))\n",
    "    max_v = max(np.max(y_true), np.max(y_pred))\n",
    "    plt.plot([min_v, max_v], [min_v, max_v], linestyle=\"--\")\n",
    "    plt.xlabel(\"True\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94753cd2-7666-46ec-9728-a46070f0e492",
   "metadata": {},
   "source": [
    "<h3 style=\"color:Blue;\">plot_residuals_hist</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2980dd1-4e80-4d24-9bcd-9decd0e3ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals_hist(y_true, y_pred, title: str, out_path: Path):\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure()\n",
    "    plt.hist(residuals, bins=30)\n",
    "    plt.xlabel(\"Residual\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef1bf17-cbd2-483b-97aa-18b60684f1e2",
   "metadata": {},
   "source": [
    "<h3 style=\"color:Blue;\">expanded_feature_names</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03c94b9b-f69e-434e-88b5-f3c01992e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanded_feature_names(preprocessor: ColumnTransformer) -> List[str]:\n",
    "    names: List[str] = []\n",
    "    for name, trans, cols in preprocessor.transformers_:\n",
    "        if name == \"remainder\" and trans == \"drop\":\n",
    "            continue\n",
    "        if hasattr(trans, \"named_steps\") and \"onehot\" in trans.named_steps:\n",
    "            ohe = trans.named_steps[\"onehot\"]\n",
    "            if hasattr(ohe, \"get_feature_names_out\"):\n",
    "                names.extend(ohe.get_feature_names_out(cols).tolist())\n",
    "            else:\n",
    "                names.extend([f\"{c}__encoded\" for c in cols])\n",
    "        else:\n",
    "            if isinstance(cols, list):\n",
    "                names.extend(cols)\n",
    "            else:\n",
    "                names.append(cols)\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb10e5e-bb5a-4d0f-a362-d9691b933dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0942d0e6-656c-4386-8e21-2a22ded0aa65",
   "metadata": {},
   "source": [
    "<h3 style=\"color:Blue;\">summarize_missing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e65f4d3-1520-4bde-b000-daea1c6c264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    miss = df.isna().sum().sort_values(ascending=False)\n",
    "    pct = (df.isna().mean().sort_values(ascending=False) * 100.0).round(2)\n",
    "    summary = pd.DataFrame({\"missing_count\": miss, \"missing_pct\": pct})\n",
    "    summary = summary[summary[\"missing_count\"] > 0]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146693dd-0fee-45a8-a38c-3ae95ff1e012",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#1E90FF; animation:colorBlink 1s infinite;\">\n",
    "üöÄ Attention: Loading CSV Data!\n",
    "</h2>\n",
    "\n",
    "<style>\n",
    "@keyframes colorBlink {\n",
    "  0% { color: #1E90FF; }\n",
    "  50% { color: #FF1493; }\n",
    "  100% { color: #1E90FF; }\n",
    "}\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7a4445-cbe0-4e84-8e97-574e89e55f81",
   "metadata": {},
   "source": [
    "<h3>\n",
    "<span style=\"animation:blink 1s infinite;\">üî¥</span>\n",
    "<span>Training in progress...</span>\n",
    "</h3>\n",
    "\n",
    "<style>\n",
    "@keyframes blink {\n",
    "  50% { opacity: 0; }\n",
    "}\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394aac19-6ea7-4f9a-8377-fb275fbf3994",
   "metadata": {},
   "source": [
    "<style>\n",
    "@keyframes blink {\n",
    "  50% { opacity: 0; }\n",
    "}\n",
    ".blink {\n",
    "  animation: blink 1s infinite;\n",
    "  color: red;\n",
    "  font-weight: bold;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h3 class=\"blink\">‚ö° Analyzing data...</h3>\n",
    "<p class=\"blink\">‚è≥ Please wait for the processing to finish.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a64c0aad-0ac8-4730-bbbc-73cfbef2600e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview:\n",
      "   Age Gender Smoking Hx Smoking Hx Radiothreapy Thyroid Function  \\\n",
      "0   27      F      No         No              No        Euthyroid   \n",
      "1   34      F      No        Yes              No        Euthyroid   \n",
      "2   30      F      No         No              No        Euthyroid   \n",
      "3   62      F      No         No              No        Euthyroid   \n",
      "4   62      F      No         No              No        Euthyroid   \n",
      "\n",
      "          Physical Examination Adenopathy       Pathology     Focality Risk  \\\n",
      "0   Single nodular goiter-left         No  Micropapillary    Uni-Focal  Low   \n",
      "1          Multinodular goiter         No  Micropapillary    Uni-Focal  Low   \n",
      "2  Single nodular goiter-right         No  Micropapillary    Uni-Focal  Low   \n",
      "3  Single nodular goiter-right         No  Micropapillary    Uni-Focal  Low   \n",
      "4          Multinodular goiter         No  Micropapillary  Multi-Focal  Low   \n",
      "\n",
      "     T   N   M Stage       Response Recurred  \n",
      "0  T1a  N0  M0     I  Indeterminate       No  \n",
      "1  T1a  N0  M0     I      Excellent       No  \n",
      "2  T1a  N0  M0     I      Excellent       No  \n",
      "3  T1a  N0  M0     I      Excellent       No  \n",
      "4  T1a  N0  M0     I      Excellent       No  \n",
      "Detected target: Recurred | Problem: classification\n",
      "Removed 19 duplicate rows\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[target_col])\n\u001b[0;32m     46\u001b[0m y \u001b[38;5;241m=\u001b[39m df[target_col]\n\u001b[1;32m---> 48\u001b[0m preprocessor, num_cols, cat_cols \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_preprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m problem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     51\u001b[0m     test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.3\u001b[39m\n",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m, in \u001b[0;36mbuild_preprocessor\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m      6\u001b[0m categorical_features \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m numeric_features]\n\u001b[0;32m      8\u001b[0m numeric_transformer \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      9\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m, StandardScaler())\n\u001b[0;32m     10\u001b[0m ])\n\u001b[1;32m---> 12\u001b[0m categorical_transformer \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monehot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)])\n\u001b[0;32m     14\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[0;32m     15\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     16\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m\"\u001b[39m, numeric_transformer, numeric_features),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preprocessor, numeric_features, categorical_features\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"File not found at {DATA_PATH}\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# -----------------------------\n",
    "# Target detection\n",
    "# -----------------------------\n",
    "if TARGET_COLUMN is None:\n",
    "    target_col, problem_type = choose_target_column(df)\n",
    "else:\n",
    "    target_col = TARGET_COLUMN\n",
    "    if pd.api.types.is_numeric_dtype(df[target_col]):\n",
    "        if df[target_col].nunique(dropna=True) > 15:\n",
    "            problem_type = \"regression\"\n",
    "        else:\n",
    "            problem_type = \"classification\"\n",
    "    else:\n",
    "        problem_type = \"classification\"\n",
    "\n",
    "print(f\"Detected target: {target_col} | Problem: {problem_type}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Basic EDA and cleaning\n",
    "# -----------------------------\n",
    "missing_summary = summarize_missing(df)\n",
    "if not missing_summary.empty:\n",
    "    print(\"\\nMissing values summary:\")\n",
    "    print(missing_summary.head(20))\n",
    "\n",
    "df = df.dropna(subset=[target_col]).copy()\n",
    "\n",
    "dup_count = df.duplicated().sum()\n",
    "if dup_count > 0:\n",
    "    df = df.drop_duplicates().copy()\n",
    "    print(f\"Removed {dup_count} duplicate rows\")\n",
    "\n",
    "# -----------------------------\n",
    "# Train / Test split\n",
    "# -----------------------------\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "preprocessor, num_cols, cat_cols = build_preprocessor(X)\n",
    "\n",
    "if problem_type == \"classification\" and y.nunique() > 1:\n",
    "    test_size = 0.2 if df.shape[0] > 50 else 0.3\n",
    "    stratify = y\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=stratify\n",
    "    )\n",
    "else:\n",
    "    test_size = 0.2 if df.shape[0] > 50 else 0.3\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "\n",
    "metrics_records: List[Dict[str, Any]] = []\n",
    "\n",
    "if problem_type == \"classification\":\n",
    "    models = {\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "        \"RandomForestClassifier\": RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "    }\n",
    "\n",
    "    for name, clf in models.items():\n",
    "        pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", clf)])\n",
    "        if y_train.nunique() > 1:\n",
    "            cv = StratifiedKFold(n_splits=min(5, y_train.value_counts().min() if y_train.value_counts().min() >= 2 else 2), shuffle=True, random_state=42)\n",
    "            try:\n",
    "                cv_acc = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "                cv_f1 = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"f1_weighted\")\n",
    "                cv_mean_acc = float(np.mean(cv_acc))\n",
    "                cv_mean_f1 = float(np.mean(cv_f1))\n",
    "            except Exception as e:\n",
    "                cv_mean_acc, cv_mean_f1 = np.nan, np.nan\n",
    "        else:\n",
    "            cv_mean_acc, cv_mean_f1 = np.nan, np.nan\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "        roc_auc = np.nan\n",
    "        if y.nunique() == 2:\n",
    "            try:\n",
    "                if hasattr(pipe.named_steps[\"model\"], \"predict_proba\"):\n",
    "                    y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "                elif hasattr(pipe.named_steps[\"model\"], \"decision_function\"):\n",
    "                    y_proba = pipe.decision_function(X_test)\n",
    "                    y_proba = (y_proba - np.min(y_proba)) / (np.max(y_proba) - np.min(y_proba) + 1e-9)\n",
    "                else:\n",
    "                    y_proba = None\n",
    "                if y_proba is not None:\n",
    "                    y_true_bin = pd.Series(y_test).astype(\"category\").cat.codes\n",
    "                    roc_auc = roc_auc_score(y_true_bin, y_proba)\n",
    "            except Exception:\n",
    "                roc_auc = np.nan\n",
    "\n",
    "        metrics_records.append({\n",
    "            \"model\": name,\n",
    "            \"cv_accuracy_mean\": cv_mean_acc,\n",
    "            \"cv_f1_weighted_mean\": cv_mean_f1,\n",
    "            \"test_accuracy\": acc,\n",
    "            \"test_precision_weighted\": prec,\n",
    "            \"test_recall_weighted\": rec,\n",
    "            \"test_f1_weighted\": f1,\n",
    "            \"test_roc_auc_binary\": roc_auc\n",
    "        })\n",
    "\n",
    "        labels = np.unique(y_test)\n",
    "        cm_path = OUTPUT_DIR / f\"confusion_matrix_{name}.png\"\n",
    "        try:\n",
    "            plt.close('all')\n",
    "            cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "            fig_cm = plt.figure()\n",
    "            disp.plot(values_format=\"d\")\n",
    "            plt.title(f\"Confusion Matrix - {name}\")\n",
    "            plt.tight_layout()\n",
    "            fig_cm.savefig(cm_path)\n",
    "            plt.show()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if (y.nunique() == 2) and not np.isnan(roc_auc):\n",
    "            try:\n",
    "                if hasattr(pipe.named_steps[\"model\"], \"predict_proba\"):\n",
    "                    y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "                elif hasattr(pipe.named_steps[\"model\"], \"decision_function\"):\n",
    "                    y_proba = pipe.decision_function(X_test)\n",
    "                    y_proba = (y_proba - np.min(y_proba)) / (np.max(y_proba) - np.min(y_proba) + 1e-9)\n",
    "                else:\n",
    "                    y_proba = None\n",
    "                if y_proba is not None:\n",
    "                    roc_path = OUTPUT_DIR / f\"roc_curve_{name}.png\"\n",
    "                    plt.close('all')\n",
    "                    fpr, tpr, _ = roc_curve(pd.Series(y_test).astype(\"category\").cat.codes, y_proba)\n",
    "                    plt.figure()\n",
    "                    plt.plot(fpr, tpr, label=\"ROC\")\n",
    "                    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "                    plt.xlabel(\"False Positive Rate\")\n",
    "                    plt.ylabel(\"True Positive Rate\")\n",
    "                    plt.title(f\"ROC Curve - {name}\")\n",
    "                    plt.legend()\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(roc_path)\n",
    "                    plt.show()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            pipe.fit(X_train, y_train)\n",
    "            names: List[str] = []\n",
    "            for tname, trans, cols in pipe.named_steps[\"prep\"].transformers_:\n",
    "                if tname == \"remainder\" and trans == \"drop\":\n",
    "                    continue\n",
    "                if hasattr(trans, \"named_steps\") and \"onehot\" in trans.named_steps:\n",
    "                    ohe = trans.named_steps[\"onehot\"]\n",
    "                    names.extend(ohe.get_feature_names_out(cols).tolist())\n",
    "                else:\n",
    "                    names.extend(cols if isinstance(cols, list) else [cols])\n",
    "            if hasattr(pipe.named_steps[\"model\"], \"feature_importances_\"):\n",
    "                fi_path = OUTPUT_DIR / f\"feature_importances_{name}.png\"\n",
    "                importances = pipe.named_steps[\"model\"].feature_importances_\n",
    "                idx = np.argsort(importances)[::-1][:min(25, len(importances))]\n",
    "                plt.close('all')\n",
    "                plt.figure()\n",
    "                plt.bar(range(len(idx)), importances[idx])\n",
    "                plt.xticks(range(len(idx)), [names[i] for i in idx], rotation=90)\n",
    "                plt.title(f\"Feature Importances - {name}\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(fi_path)\n",
    "                plt.show()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        model_path = OUTPUT_DIR / f\"model_{name}.joblib\"\n",
    "        try:\n",
    "            joblib.dump(pipe, model_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_records).sort_values(by=\"test_f1_weighted\", ascending=False)\n",
    "    best_model_name = metrics_df.iloc[0][\"model\"]\n",
    "    # Correction - Romi Lemes\n",
    "    best_pipe = Pipeline(steps=[(\"prep\", preprocessor),\n",
    "                               (\"model\", models[best_model_name])])\n",
    "    best_pipe.fit(X_train, y_train)\n",
    "    y_pred_best = best_pipe.predict(X_test)\n",
    "    report_text = classification_report(y_test, y_pred_best, zero_division=0)\n",
    "    report_path = OUTPUT_DIR / \"classification_report.txt\"\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report_text)\n",
    "\n",
    "    metrics_csv_path = OUTPUT_DIR / \"metrics_classification.csv\"\n",
    "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "\n",
    "else:\n",
    "    models = {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"RandomForestRegressor\": RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "    }\n",
    "    for name, reg in models.items():\n",
    "        # last Version\n",
    "        # pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"model\", reg)])\n",
    "        # New Version - 20251030\n",
    "        pipe = Pipeline(steps=[\n",
    "            (\"prep\", preprocessor),\n",
    "            (\"model\", LogisticRegression(max_iter=1000))\n",
    "        ])\n",
    "\n",
    "        cv = KFold(n_splits=5 if len(X_train) >= 50 else 3, shuffle=True, random_state=42)\n",
    "        try:\n",
    "            cv_r2 = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"r2\")\n",
    "            cv_mae = -cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"neg_mean_absolute_error\")\n",
    "            cv_mean_r2 = float(np.mean(cv_r2))\n",
    "            cv_mean_mae = float(np.mean(cv_mae))\n",
    "        except Exception:\n",
    "            cv_mean_r2, cv_mean_mae = np.nan, np.nan\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "\n",
    "        metrics_records.append({\n",
    "            \"model\": name,\n",
    "            \"cv_r2_mean\": cv_mean_r2,\n",
    "            \"cv_mae_mean\": cv_mean_mae,\n",
    "            \"test_r2\": r2,\n",
    "            \"test_mae\": mae,\n",
    "            \"test_rmse\": rmse\n",
    "        })\n",
    "\n",
    "        scatter_path = OUTPUT_DIR / f\"true_vs_pred_{name}.png\"\n",
    "        try:\n",
    "            plt.close('all')\n",
    "            plt.figure()\n",
    "            plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "            min_v = min(np.min(y_test), np.min(y_pred))\n",
    "            max_v = max(np.max(y_test), np.max(y_pred))\n",
    "            plt.plot([min_v, max_v], [min_v, max_v], linestyle=\"--\")\n",
    "            plt.xlabel(\"True\")\n",
    "            plt.ylabel(\"Predicted\")\n",
    "            plt.title(f\"True vs Predicted - {name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(scatter_path)\n",
    "            plt.show()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        resid_path = OUTPUT_DIR / f\"residuals_hist_{name}.png\"\n",
    "        try:\n",
    "            plt.close('all')\n",
    "            residuals = y_test - y_pred\n",
    "            plt.figure()\n",
    "            plt.hist(residuals, bins=30)\n",
    "            plt.xlabel(\"Residual\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.title(f\"Residuals Histogram - {name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(resid_path)\n",
    "            plt.show()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            pipe.fit(X_train, y_train)\n",
    "            names: List[str] = []\n",
    "            for tname, trans, cols in pipe.named_steps[\"prep\"].transformers_:\n",
    "                if tname == \"remainder\" and trans == \"drop\":\n",
    "                    continue\n",
    "                if hasattr(trans, \"named_steps\") and \"onehot\" in trans.named_steps:\n",
    "                    ohe = trans.named_steps[\"onehot\"]\n",
    "                    names.extend(ohe.get_feature_names_out(cols).tolist())\n",
    "                else:\n",
    "                    names.extend(cols if isinstance(cols, list) else [cols])\n",
    "            if hasattr(pipe.named_steps[\"model\"], \"feature_importances_\"):\n",
    "                fi_path = OUTPUT_DIR / f\"feature_importances_{name}.png\"\n",
    "                importances = pipe.named_steps[\"model\"].feature_importances_\n",
    "                idx = np.argsort(importances)[::-1][:min(25, len(importances))]\n",
    "                plt.close('all')\n",
    "                plt.figure()\n",
    "                plt.bar(range(len(idx)), importances[idx])\n",
    "                plt.xticks(range(len(idx)), [names[i] for i in idx], rotation=90)\n",
    "                plt.title(f\"Feature Importances - {name}\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(fi_path)\n",
    "                plt.show()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        model_path = OUTPUT_DIR / f\"model_{name}.joblib\"\n",
    "        try:\n",
    "            joblib.dump(pipe, model_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_records).sort_values(by=\"test_r2\", ascending=False)\n",
    "    metrics_csv_path = OUTPUT_DIR / \"metrics_regression.csv\"\n",
    "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "\n",
    "print(\"Done. Outputs in:\", OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
